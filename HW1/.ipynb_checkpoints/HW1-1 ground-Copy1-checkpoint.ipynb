{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "#from model import *\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class NN(object):\n",
    "    def __init__(self, layers = [10 , 20, 1], activations=['sigmoid', 'relu'], usage = 'regression'):\n",
    "        assert(len(layers) == len(activations)+1)\n",
    "        self.layers = layers\n",
    "        self.activations = activations\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        self.usage = usage\n",
    "        for i in range(len(layers)-1):\n",
    "            self.weights.append(np.random.randn(layers[i+1], layers[i])*0.1) #the *0.1 is impmortant\n",
    "            self.biases.append(np.random.randn(layers[i+1], 1)*0.1)\n",
    "\n",
    "    def feedforward(self, x): #x = dim*num\n",
    "        ai = np.copy(x)\n",
    "        z_s = []\n",
    "        a_s = [ai]\n",
    "        for i in range(len(self.weights)):\n",
    "            #activation_function = self.AF(self.activations[i])\n",
    "            z_s.append(self.weights[i].dot(ai) + self.biases[i])\n",
    "            ai = self.AF(self.activations[i])(z_s[-1])\n",
    "            a_s.append(ai)\n",
    "        return (z_s, a_s)\n",
    "\n",
    "    def backpropagation(self,y, z_s, a_s): #y = 1*num\n",
    "        dw = []  # dC/dW\n",
    "        db = []  # dC/dB\n",
    "        deltas = [None] * len(self.weights)  # delta = dC/dZ, error for each layer\n",
    "\n",
    "        #out delta measurement =\n",
    "        delta_out = y- a_s[-1]\n",
    "        #last layer delta\n",
    "        deltas[-1] = delta_out*(self.dAF(self.activations[-1]))(z_s[-1])\n",
    "        #backpro\n",
    "        for i in reversed(range(len(deltas)-1)):\n",
    "            deltas[i] = self.weights[i+1].T.dot(deltas[i+1])*(self.dAF(self.activations[i])(z_s[i]))\n",
    "        batch_size = y.shape[1]\n",
    "        db = [d.dot(np.ones((batch_size,1)))/float(batch_size) for d in deltas]\n",
    "        dw = [d.dot(a_s[i].T)/float(batch_size) for i,d in enumerate(deltas)]\n",
    "        #db = [d.dot(np.ones((batch_size,1))) for d in deltas]\n",
    "        #dw = [d.dot(a_s[i].T) for i,d in enumerate(deltas)]\n",
    "        # return the derivitives respect to weight matrix and biases\n",
    "        #print(db)\n",
    "        #print(dw)\n",
    "        return dw, db\n",
    "\n",
    "    def train(self, x, y, batch_size=10, epochs=100, lr = 0.1): #x = num*dim #y = num*dim\n",
    "        #record cost by epchos\n",
    "        learning_curve = []\n",
    "\n",
    "        #mini batch\n",
    "        #assert(x.shape[0] >= batch_size*epochs)\n",
    "        indices = np.arange(x.shape[0])#debug if 0\n",
    "        np.random.shuffle(indices)\n",
    "        x = x[indices]\n",
    "        y = y[indices]\n",
    "\n",
    "        for e in range(epochs):\n",
    "            i=0\n",
    "            #print(\"len y  \", len(y))\n",
    "            while(i<len(y)):\n",
    "                x_batch = x[i:i+batch_size]\n",
    "                y_batch = y[i:i+batch_size]\n",
    "                x_batch = x_batch.T\n",
    "                y_batch = y_batch.T\n",
    "                #print(x_batch.shape)\n",
    "                #print(y_batch.shape)\n",
    "                i += batch_size\n",
    "                z_s, a_s = self.feedforward(x_batch)\n",
    "                dw, db = self.backpropagation(y_batch, z_s, a_s)\n",
    "                self.weights = [wi+lr*dwi for wi,dwi in  zip(self.weights, dw)]\n",
    "                self.biases = [bi+lr*dbi for bi,dbi in  zip(self.biases, db)]\n",
    "                loss = self.J(self.usage)(a_s[-1],y_batch)\n",
    "            #if(e%(epochs/10)== 0):\n",
    "            learning_curve.append(loss) #to expand\n",
    "            #print(\"loss = {}\".format(np.linalg.norm(a_s[-1]-y_batch))) #to expand\n",
    "        return learning_curve\n",
    "\n",
    "\n",
    "    def calc_error(self, test_X, test_y): #num*dim\n",
    "        _, a_s = self.feedforward(test_X.T)\n",
    "        return  self.J(self.usage)(a_s[-1], test_y.T)\n",
    "\n",
    "    def prediction(self, X): #num*dim\n",
    "        _, a_s = self.feedforward(X.T)\n",
    "        return a_s[-1]\n",
    "\n",
    "    @staticmethod\n",
    "    def AF(name):\n",
    "        if(name == 'sigmoid'):\n",
    "            def sig(x):\n",
    "                x = np.clip(x , -500, 500)\n",
    "                return np.exp(x)/(1+np.exp(x))\n",
    "            return sig\n",
    "        elif(name == 'linear'):\n",
    "            return lambda x : x\n",
    "        elif(name == 'relu'):\n",
    "            def relu(x):\n",
    "                y = np.copy(x)\n",
    "                y[y<0] = 0\n",
    "                return y\n",
    "            return relu\n",
    "        else:\n",
    "            print('unknown activation function => linear')\n",
    "            return lambda x: x\n",
    "\n",
    "    @staticmethod\n",
    "    def dAF(name):\n",
    "        if(name == 'sigmoid'):\n",
    "            def dsig(x):\n",
    "                x = np.clip(x , -500, 500)\n",
    "                sigx = np.exp(x)/(1+np.exp(x))\n",
    "                return sigx*(1-sigx)\n",
    "            return dsig\n",
    "        elif(name == 'linear'):\n",
    "            return lambda x: 1\n",
    "        elif(name == 'relu'):\n",
    "            def drelu(x):\n",
    "                y = np.copy(x)\n",
    "                y[y>=0] = 1\n",
    "                y[y<0] = 0\n",
    "                return y\n",
    "            return drelu\n",
    "        else:\n",
    "            print('unknown activation function => linear derivative')\n",
    "            return lambda x: 1\n",
    "\n",
    "    @staticmethod\n",
    "    def dJ(name):\n",
    "        if(name == 'regression'):\n",
    "            return lambda x, y: y-x\n",
    "        if(name == 'classification'):\n",
    "            return lambda x, y: np.divide(y, x) - np.divide(1 - y, 1 - x)\n",
    "        else:\n",
    "            print('unknown usage => regression')\n",
    "            return lambda x, y: y-x\n",
    "\n",
    "    @staticmethod\n",
    "    def J(name):\n",
    "        if(name == 'regression'):\n",
    "            return lambda x, y: np.sqrt(np.linalg.norm(y-x)/max(y.shape[0], y.shape[1])) #RMS\n",
    "        if(name == 'classification'):\n",
    "            return lambda x, y: np.divide(y, x) - np.divide(1 - y, 1 - x)\n",
    "        else:\n",
    "            print('unknown usage => regression')\n",
    "            return lambda x, y: np.sqrt(np.linalg.norm(y-x)/np.max(y.shape[0], y.shape[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Relative Compactness</th>\n",
       "      <th>Surface Area</th>\n",
       "      <th>Wall Area</th>\n",
       "      <th>Roof Area</th>\n",
       "      <th>Overall Height</th>\n",
       "      <th>Orientation</th>\n",
       "      <th>Glazing Area</th>\n",
       "      <th>Glazing Area Distribution</th>\n",
       "      <th>Heating Load</th>\n",
       "      <th>Cooling Load</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.98</td>\n",
       "      <td>514.5</td>\n",
       "      <td>294.0</td>\n",
       "      <td>110.25</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>15.55</td>\n",
       "      <td>21.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.98</td>\n",
       "      <td>514.5</td>\n",
       "      <td>294.0</td>\n",
       "      <td>110.25</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>15.55</td>\n",
       "      <td>21.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.98</td>\n",
       "      <td>514.5</td>\n",
       "      <td>294.0</td>\n",
       "      <td>110.25</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>15.55</td>\n",
       "      <td>21.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.98</td>\n",
       "      <td>514.5</td>\n",
       "      <td>294.0</td>\n",
       "      <td>110.25</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>15.55</td>\n",
       "      <td>21.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.90</td>\n",
       "      <td>563.5</td>\n",
       "      <td>318.5</td>\n",
       "      <td>122.50</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>20.84</td>\n",
       "      <td>28.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>763</td>\n",
       "      <td>0.64</td>\n",
       "      <td>784.0</td>\n",
       "      <td>343.0</td>\n",
       "      <td>220.50</td>\n",
       "      <td>3.5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>5</td>\n",
       "      <td>17.88</td>\n",
       "      <td>21.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>764</td>\n",
       "      <td>0.62</td>\n",
       "      <td>808.5</td>\n",
       "      <td>367.5</td>\n",
       "      <td>220.50</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>5</td>\n",
       "      <td>16.54</td>\n",
       "      <td>16.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>765</td>\n",
       "      <td>0.62</td>\n",
       "      <td>808.5</td>\n",
       "      <td>367.5</td>\n",
       "      <td>220.50</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>5</td>\n",
       "      <td>16.44</td>\n",
       "      <td>17.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>766</td>\n",
       "      <td>0.62</td>\n",
       "      <td>808.5</td>\n",
       "      <td>367.5</td>\n",
       "      <td>220.50</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>5</td>\n",
       "      <td>16.48</td>\n",
       "      <td>16.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>767</td>\n",
       "      <td>0.62</td>\n",
       "      <td>808.5</td>\n",
       "      <td>367.5</td>\n",
       "      <td>220.50</td>\n",
       "      <td>3.5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>5</td>\n",
       "      <td>16.64</td>\n",
       "      <td>16.03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>768 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Relative Compactness  Surface Area  Wall Area  Roof Area  Overall Height  \\\n",
       "0                    0.98         514.5      294.0     110.25             7.0   \n",
       "1                    0.98         514.5      294.0     110.25             7.0   \n",
       "2                    0.98         514.5      294.0     110.25             7.0   \n",
       "3                    0.98         514.5      294.0     110.25             7.0   \n",
       "4                    0.90         563.5      318.5     122.50             7.0   \n",
       "..                    ...           ...        ...        ...             ...   \n",
       "763                  0.64         784.0      343.0     220.50             3.5   \n",
       "764                  0.62         808.5      367.5     220.50             3.5   \n",
       "765                  0.62         808.5      367.5     220.50             3.5   \n",
       "766                  0.62         808.5      367.5     220.50             3.5   \n",
       "767                  0.62         808.5      367.5     220.50             3.5   \n",
       "\n",
       "     Orientation  Glazing Area  Glazing Area Distribution  Heating Load  \\\n",
       "0              2           0.0                          0         15.55   \n",
       "1              3           0.0                          0         15.55   \n",
       "2              4           0.0                          0         15.55   \n",
       "3              5           0.0                          0         15.55   \n",
       "4              2           0.0                          0         20.84   \n",
       "..           ...           ...                        ...           ...   \n",
       "763            5           0.4                          5         17.88   \n",
       "764            2           0.4                          5         16.54   \n",
       "765            3           0.4                          5         16.44   \n",
       "766            4           0.4                          5         16.48   \n",
       "767            5           0.4                          5         16.64   \n",
       "\n",
       "     Cooling Load  \n",
       "0           21.33  \n",
       "1           21.33  \n",
       "2           21.33  \n",
       "3           21.33  \n",
       "4           28.28  \n",
       "..            ...  \n",
       "763         21.40  \n",
       "764         16.88  \n",
       "765         17.11  \n",
       "766         16.61  \n",
       "767         16.03  \n",
       "\n",
       "[768 rows x 10 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#preprocessing\n",
    "\n",
    "df = pd.read_csv(\"EnergyEfficiency_data.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_onehot(df, name):\n",
    "    A = df[name].values\n",
    "    n = A.shape[0]\n",
    "    onehot_A = np.zeros((n,max(A)-min(A)+1))\n",
    "    onehot_A[np.arange(n), A-min(A)] = 1\n",
    "    return onehot_A\n",
    "\n",
    "def normalize(X):\n",
    "    s = [ np.mean(dim) for dim in X.T]\n",
    "    X = np.asarray([np.divide(x, s) for x in X])\n",
    "    return X\n",
    "\n",
    "O = get_onehot(df, \"Orientation\")\n",
    "G = get_onehot(df, \"Glazing Area Distribution\")\n",
    "y = df[\"Heating Load\"].values.reshape((-1,1))\n",
    "y.shape\n",
    "Other = df.drop(['Orientation', 'Glazing Area Distribution', \"Heating Load\"], axis=1).values\n",
    "\n",
    "X = np.c_[normalize(Other), O, G]\n",
    "assert(X.shape[1] == O.shape[1]+G.shape[1]+Other.shape[1])\n",
    "\n",
    "def partition(X, y, ratio=0.7):\n",
    "    n = X.shape[0]\n",
    "    indices = np.arange(n)\n",
    "    np.random.shuffle(indices)\n",
    "    X = X[indices]\n",
    "    y = y[indices]\n",
    "    p = int(n*ratio)\n",
    "    train_X = X[:p]\n",
    "    test_X = X[p:]\n",
    "    train_y = y[:p]\n",
    "    test_y = y[p:]\n",
    "    return train_X, train_y, test_X, test_y\n",
    "\n",
    "train_X, train_y, test_X, test_y = partition(X, y, ratio=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NN([17, 15, 5, 1],activations=['sigmoid', 'sigmoid', 'relu'], usage = 'regression')\n",
    "\n",
    "learning_curve = nn.train(train_X, train_y, epochs=70, batch_size=10, lr = .1)\n",
    "\n",
    "train_RMS = nn.calc_error(train_X, train_y)\n",
    "test_RMS = nn.calc_error(test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, a_s = nn.feedforward(X.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "savefilename = \"savenn\"\n",
    "with open(savefilename, 'wb') as fo:\n",
    "    pickle.dump(nn, fo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x122fe8090>]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(len(learning_curve)), learning_curve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23558955736275677 0.30085837873996324\n"
     ]
    }
   ],
   "source": [
    "print(train_RMS, test_RMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
